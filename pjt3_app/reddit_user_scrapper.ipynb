{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import gensim.utils\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pickle\n",
    "\n",
    "import praw\n",
    "from praw.models import MoreComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>post_text</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>total_comments</th>\n",
       "      <th>post_url</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_type</th>\n",
       "      <th>title_&amp;_text</th>\n",
       "      <th>title_text_stemmed</th>\n",
       "      <th>title_text_lemmatized</th>\n",
       "      <th>trending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Daily Fasting Check-in!</td>\n",
       "      <td>* **Type** of fast (water, juice, smoking, etc...</td>\n",
       "      <td>16o7z6r</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.reddit.com/r/intermittentfasting/c...</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>hot</td>\n",
       "      <td>Daily Fasting Check-in! * **Type** of fast (wa...</td>\n",
       "      <td>['daili', 'checkin', 'type', 'fast', 'water', ...</td>\n",
       "      <td>['daily', 'checkin', 'type', 'fast', 'water', ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I decided who I wanted to be and I became her üíÖüèΩ</td>\n",
       "      <td>So a little background: I‚Äôm 39, have birthed t...</td>\n",
       "      <td>16ntqoy</td>\n",
       "      <td>1176</td>\n",
       "      <td>36</td>\n",
       "      <td>https://i.redd.it/fclkjnwhmgpb1.jpg</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>hot</td>\n",
       "      <td>I decided who I wanted to be and I became her ...</td>\n",
       "      <td>['decid', 'want', 'becam', 'littl', 'backgroun...</td>\n",
       "      <td>['decided', 'wanted', 'became', 'little', 'bac...</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Some photos from a past vacation came up as a ...</td>\n",
       "      <td>I remember being miserable and insecure the en...</td>\n",
       "      <td>16ni914</td>\n",
       "      <td>1505</td>\n",
       "      <td>77</td>\n",
       "      <td>https://www.reddit.com/gallery/16ni914</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>hot</td>\n",
       "      <td>Some photos from a past vacation came up as a ...</td>\n",
       "      <td>['photo', 'past', 'vacat', 'came', 'memori', '...</td>\n",
       "      <td>['photo', 'past', 'vacation', 'came', 'memory'...</td>\n",
       "      <td>115885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anybody find IF, lose weight, and then lose mo...</td>\n",
       "      <td>I know I am an idiot.</td>\n",
       "      <td>16nuqx9</td>\n",
       "      <td>198</td>\n",
       "      <td>78</td>\n",
       "      <td>https://www.reddit.com/r/intermittentfasting/c...</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>hot</td>\n",
       "      <td>Anybody find IF, lose weight, and then lose mo...</td>\n",
       "      <td>['anybodi', 'find', 'lose', 'weight', 'lose', ...</td>\n",
       "      <td>['anybody', 'find', 'lose', 'weight', 'lose', ...</td>\n",
       "      <td>15444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 and a half months of IF</td>\n",
       "      <td>From 234 to 211 in 2.5 months. It works! Once ...</td>\n",
       "      <td>16nuxqs</td>\n",
       "      <td>180</td>\n",
       "      <td>12</td>\n",
       "      <td>https://i.redd.it/30yqmtsdvgpb1.jpg</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>hot</td>\n",
       "      <td>2 and a half months of IF From 234 to 211 in 2...</td>\n",
       "      <td>['2', 'half', 'month', '234', '211', '25', 'mo...</td>\n",
       "      <td>['2', 'half', 'month', '234', '211', '25', 'mo...</td>\n",
       "      <td>2160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                            Daily Fasting Check-in!   \n",
       "1   I decided who I wanted to be and I became her üíÖüèΩ   \n",
       "2  Some photos from a past vacation came up as a ...   \n",
       "3  Anybody find IF, lose weight, and then lose mo...   \n",
       "4                          2 and a half months of IF   \n",
       "\n",
       "                                           post_text       id  score  \\\n",
       "0  * **Type** of fast (water, juice, smoking, etc...  16o7z6r      1   \n",
       "1  So a little background: I‚Äôm 39, have birthed t...  16ntqoy   1176   \n",
       "2  I remember being miserable and insecure the en...  16ni914   1505   \n",
       "3                              I know I am an idiot.  16nuqx9    198   \n",
       "4  From 234 to 211 in 2.5 months. It works! Once ...  16nuxqs    180   \n",
       "\n",
       "   total_comments                                           post_url  \\\n",
       "0               2  https://www.reddit.com/r/intermittentfasting/c...   \n",
       "1              36                https://i.redd.it/fclkjnwhmgpb1.jpg   \n",
       "2              77             https://www.reddit.com/gallery/16ni914   \n",
       "3              78  https://www.reddit.com/r/intermittentfasting/c...   \n",
       "4              12                https://i.redd.it/30yqmtsdvgpb1.jpg   \n",
       "\n",
       "             subreddit post_type  \\\n",
       "0  intermittentfasting       hot   \n",
       "1  intermittentfasting       hot   \n",
       "2  intermittentfasting       hot   \n",
       "3  intermittentfasting       hot   \n",
       "4  intermittentfasting       hot   \n",
       "\n",
       "                                        title_&_text  \\\n",
       "0  Daily Fasting Check-in! * **Type** of fast (wa...   \n",
       "1  I decided who I wanted to be and I became her ...   \n",
       "2  Some photos from a past vacation came up as a ...   \n",
       "3  Anybody find IF, lose weight, and then lose mo...   \n",
       "4  2 and a half months of IF From 234 to 211 in 2...   \n",
       "\n",
       "                                  title_text_stemmed  \\\n",
       "0  ['daili', 'checkin', 'type', 'fast', 'water', ...   \n",
       "1  ['decid', 'want', 'becam', 'littl', 'backgroun...   \n",
       "2  ['photo', 'past', 'vacat', 'came', 'memori', '...   \n",
       "3  ['anybodi', 'find', 'lose', 'weight', 'lose', ...   \n",
       "4  ['2', 'half', 'month', '234', '211', '25', 'mo...   \n",
       "\n",
       "                               title_text_lemmatized  trending  \n",
       "0  ['daily', 'checkin', 'type', 'fast', 'water', ...         2  \n",
       "1  ['decided', 'wanted', 'became', 'little', 'bac...     42336  \n",
       "2  ['photo', 'past', 'vacation', 'came', 'memory'...    115885  \n",
       "3  ['anybody', 'find', 'lose', 'weight', 'lose', ...     15444  \n",
       "4  ['2', 'half', 'month', '234', '211', '25', 'mo...      2160  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('../data/reddits.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2489 entries, 0 to 2488\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   title                  2489 non-null   object\n",
      " 1   post_text              2489 non-null   object\n",
      " 2   id                     2489 non-null   object\n",
      " 3   score                  2489 non-null   int64 \n",
      " 4   total_comments         2489 non-null   int64 \n",
      " 5   post_url               2489 non-null   object\n",
      " 6   subreddit              2489 non-null   object\n",
      " 7   post_type              2489 non-null   object\n",
      " 8   title_&_text           2489 non-null   object\n",
      " 9   title_text_stemmed     2489 non-null   object\n",
      " 10  title_text_lemmatized  2489 non-null   object\n",
      " 11  trending               2489 non-null   int64 \n",
      " 12  subreddit_binarized    2489 non-null   int64 \n",
      "dtypes: int64(4), object(9)\n",
      "memory usage: 252.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: subreddit_binarized, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binarize 'subreddit' for modelling\n",
    "# 'AnorexiaNervosa' = 0\n",
    "# 'intermittentfasting' = 1\n",
    "df['subreddit_binarized'] = df['subreddit'].map({'AnorexiaNervosa': 0, 'intermittentfasting': 1})\n",
    "df['subreddit_binarized'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble features (X) and target (y)\n",
    "X = df['title_text_stemmed'].tolist()\n",
    "y = df['subreddit_binarized'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize data using TF-IDF then train model using Bernoulli\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Instantiate TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# Instantiate the Bernoulli model\n",
    "BernNB = BernoulliNB(binarize=0.1)\n",
    "\n",
    "# Fit the model\n",
    "BernNB.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on both training and test data\n",
    "y_pred_train = BernNB.predict(X_train_resampled)\n",
    "y_pred_test = BernNB.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9397590361445783\n"
     ]
    }
   ],
   "source": [
    "# Print train and test scores\n",
    "print(accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to the pickle file\n",
    "with open('../data/bernoulli_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(BernNB, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vectorizer to the pickle file\n",
    "with open('../data/tfidf_vectorizer.pkl', 'wb') as vectorizer_file:\n",
    "    pickle.dump(tfidf, vectorizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Reddit API credentials\n",
    "reddit_client_id = \"-rUx3v29zVVe7aMPZtnPCA\"\n",
    "reddit_client_secret = \"rNQ7a89ilfDRLSAPEZ-3tmB9ZgwScA\"\n",
    "reddit_user_agent = \"39 SIR Scraper\"\n",
    "\n",
    "# Initialize the Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=reddit_client_id,\n",
    "    client_secret=reddit_client_secret,\n",
    "    user_agent=reddit_user_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape Reddit user's posts\n",
    "def scrape_reddit_user_posts(username, num_posts=100):\n",
    "    posts_dict = {\n",
    "        \"title\": [],\n",
    "        \"post_text\": [],\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get the Reddit user instance\n",
    "        user = reddit.redditor(username)\n",
    "\n",
    "        # Iterate through the user's submissions (posts)\n",
    "        for submission in user.submissions.top(limit=num_posts):\n",
    "            # Append the title of each post to the list\n",
    "            posts_dict['title'].append(submission.title)\n",
    "            posts_dict['post_text'].append(submission.selftext)\n",
    "        # Convert the dict to a dataframe\n",
    "        posts_dict_df = pd.DataFrame(posts_dict)\n",
    "        return posts_dict_df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapped 47 posts from Jon_Henderson_Music\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>post_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IF, keto, and exercise. I'm taking my life back.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The difference a year can make.</td>\n",
       "      <td>Intermittent fasting changed my life. This com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5 pounds away from goal! So thankful for thi...</td>\n",
       "      <td>Could barely fit in those shorts in January. N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Have Boomers become absentee grandparents?</td>\n",
       "      <td>I get texts from my Mom from time to time, \"wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M/34/5‚Äô7‚Äù [205lbs to 155lbs] (8 months) Taking...</td>\n",
       "      <td>Trying to get back to my high school tennis we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   IF, keto, and exercise. I'm taking my life back.   \n",
       "1                    The difference a year can make.   \n",
       "2  1.5 pounds away from goal! So thankful for thi...   \n",
       "3         Have Boomers become absentee grandparents?   \n",
       "4  M/34/5‚Äô7‚Äù [205lbs to 155lbs] (8 months) Taking...   \n",
       "\n",
       "                                           post_text  \n",
       "0                                                     \n",
       "1  Intermittent fasting changed my life. This com...  \n",
       "2  Could barely fit in those shorts in January. N...  \n",
       "3  I get texts from my Mom from time to time, \"wh...  \n",
       "4  Trying to get back to my high school tennis we...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the scrapping function on the user\n",
    "name = \"Jon_Henderson_Music\"\n",
    "user_posts = scrape_reddit_user_posts(name, num_posts=100)\n",
    "user_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean data\n",
    "\n",
    "def clean_data(text_df):\n",
    "    # Merge title and post_text columns\n",
    "    text_df['title_text'] = text_df['title'] + ' ' + text_df['post_text']\n",
    "\n",
    "    # Remove rows with null values in the 'title_text' column\n",
    "    text_df.dropna(subset=['title_text'], inplace=True)\n",
    "\n",
    "    # Remove punctuations and tokenize using the built in cleaner in gensim\n",
    "    text_df['title_text'] = text_df['title_text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "\n",
    "    # Spply stemming and stopwords exclusion within the same step\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    ps = nltk.PorterStemmer()\n",
    "    for idx in text_df.index:\n",
    "        text_df['title_text'][idx] = [ps.stem(word) for word in text_df['title_text'][idx] if word not in stopwords]\n",
    "\n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>post_text</th>\n",
       "      <th>title_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IF, keto, and exercise. I'm taking my life back.</td>\n",
       "      <td></td>\n",
       "      <td>[keto, exercis, take, life, back]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The difference a year can make.</td>\n",
       "      <td>Intermittent fasting changed my life. This com...</td>\n",
       "      <td>[differ, year, make, intermitt, fast, chang, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5 pounds away from goal! So thankful for thi...</td>\n",
       "      <td>Could barely fit in those shorts in January. N...</td>\n",
       "      <td>[pound, away, goal, thank, commun, motiv, gave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Have Boomers become absentee grandparents?</td>\n",
       "      <td>I get texts from my Mom from time to time, \"wh...</td>\n",
       "      <td>[boomer, becom, absente, grandpar, get, text, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M/34/5‚Äô7‚Äù [205lbs to 155lbs] (8 months) Taking...</td>\n",
       "      <td>Trying to get back to my high school tennis we...</td>\n",
       "      <td>[lb, lb, month, take, life, back, tri, get, ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   IF, keto, and exercise. I'm taking my life back.   \n",
       "1                    The difference a year can make.   \n",
       "2  1.5 pounds away from goal! So thankful for thi...   \n",
       "3         Have Boomers become absentee grandparents?   \n",
       "4  M/34/5‚Äô7‚Äù [205lbs to 155lbs] (8 months) Taking...   \n",
       "\n",
       "                                           post_text  \\\n",
       "0                                                      \n",
       "1  Intermittent fasting changed my life. This com...   \n",
       "2  Could barely fit in those shorts in January. N...   \n",
       "3  I get texts from my Mom from time to time, \"wh...   \n",
       "4  Trying to get back to my high school tennis we...   \n",
       "\n",
       "                                          title_text  \n",
       "0                  [keto, exercis, take, life, back]  \n",
       "1  [differ, year, make, intermitt, fast, chang, l...  \n",
       "2  [pound, away, goal, thank, commun, motiv, gave...  \n",
       "3  [boomer, becom, absente, grandpar, get, text, ...  \n",
       "4  [lb, lb, month, take, life, back, tri, get, ba...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the user posts using the cleaning function\n",
    "clean_data(user_posts)\n",
    "user_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted subreddit: r/intermittentfasting\n",
      "Weighted confidence score: 0.8976315828739463\n"
     ]
    }
   ],
   "source": [
    "# Make prediction using the pickle file\n",
    "\n",
    "# Load the trained model and vectorizer from pickle files\n",
    "with open('../data/bernoulli_model.pkl', 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)\n",
    "\n",
    "with open('../data/tfidf_vectorizer.pkl', 'rb') as vectorizer_file:\n",
    "    loaded_vectorizer = pickle.load(vectorizer_file)\n",
    "\n",
    "# Vectorize the new data using the same vectorizer used during training\n",
    "X_new = loaded_vectorizer.transform(user_posts['title_text'].apply(lambda x: ', '.join(x)))\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = loaded_model.predict(X_new)\n",
    "\n",
    "# Get probability estimates for the predictions\n",
    "confidence_scores = loaded_model.predict_proba(X_new)\n",
    "\n",
    "# Count the occurrences of each class label in the predictions\n",
    "counts = np.bincount(predictions)\n",
    "\n",
    "# Calculate the weighted average of confidence scores\n",
    "weighted_confidence_scores = np.zeros_like(confidence_scores[0])\n",
    "for i, prediction in enumerate(predictions):\n",
    "    weighted_confidence_scores += confidence_scores[i] * (1 / counts[prediction])\n",
    "\n",
    "# Define the class labels\n",
    "class_labels = ['r/AnorexiaNervosa', 'r/intermittentfasting']\n",
    "\n",
    "# Determine the consolidated prediction label\n",
    "consolidated_prediction_label = class_labels[np.argmax(counts)]\n",
    "\n",
    "# Combine the consolidated prediction label and the weighted confidence scores\n",
    "consolidated_prediction = {\n",
    "    \"label\": consolidated_prediction_label,\n",
    "    \"confidence_scores\": weighted_confidence_scores.tolist()\n",
    "}\n",
    "\n",
    "print(f\"Predicted subreddit: {consolidated_prediction_label}\")\n",
    "print(f\"Weighted confidence score: {weighted_confidence_scores.tolist()[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [47, 498]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/m.farhanrais/Documents/GitHub/DSI-SG-39/My Projects/Project 3/code/reddit_user_scrapper.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/m.farhanrais/Documents/GitHub/DSI-SG-39/My%20Projects/Project%203/code/reddit_user_scrapper.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(accuracy_score(predictions, y_test))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:192\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m validate_parameter_constraints(\n\u001b[1;32m    188\u001b[0m     parameter_constraints, params, caller_name\u001b[39m=\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    191\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    193\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    194\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    199\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    201\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    202\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:221\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    222\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    223\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     60\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[39m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[1;32m     87\u001b[0m     type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m     type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [47, 498]"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(predictions, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
